%%This is a very basic article template.
%%There is just one section and two subsections.
\documentclass{article}

\usepackage{iclr2016_conference,times}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{parskip}
\usepackage{subcaption}
\usepackage{xcolor} 
\usepackage{cleveref}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{graphicx}
\newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}
\DeclareMathOperator*{\argmin}{argmin}
\title{TTIC 31210 Homework 3 \\ Spring 2017}

\author{Hao Jiang}
\begin{document}

\maketitle
\section{Tasks}
\subsection{Understanding HMMs}
If the HMM does not include the generation of the EOS label, the model described in the question
will no longer be describing the probability of a single sequence of length $T$, but the probability
of all sequences of length $>= T$, which begins with symbol $x_{1:T}$ and tagged with $y_{1:T}$

\subsection{Supervised Learning for HMMs:Implementation}

The implementation is in \texttt{hmm.py}. The top 10 words for ADJ and their probability are listed 
in \Cref{tab:top10adj} and top 5 tag following PROPN are listed in \Cref{tab:top5propn}
\begin{table}
\centering
\begin{tabular}{c|c}
other & 0.001310 \\\hline
good & 0.001227 \\\hline
new & 0.000953 \\\hline
more & 0.000806 \\\hline
great & 0.000777 \\\hline
many & 0.000777 \\\hline
best & 0.000655 \\\hline
same & 0.000645 \\\hline
sure & 0.000542 \\\hline
last & 0.000523 \\
\end{tabular}
\caption{Top 10 Words for ADJ}
\label{tab:top10adj}
\end{table}
\renewcommand{\arraystretch}{1.5}
\begin{table}
\centering
\begin{tabular}{c|c}
PROPN & 0.017963\\\hline
PUNCT & 0.016391\\\hline
NOUN & 0.005170\\\hline
ADP & 0.004191\\\hline
VERB & 0.004186\\\hline
\end{tabular}
\caption{Top 5 tags following PROPN}
\label{tab:top5propn}
\end{table}

\subsection{Gibbs Sampling for Decoding}
\textbf{a)}

In a HMM model we have the following properties:
\begin{align*}
&Y_t \indep Y_{1:t-2} | Y_{t-1}\\
&Y_t \indep Y_{t+2:T}| Y_{t+1}\\
&Y_t \indep X_i | Y_i (i \neq t)
\end{align*}
Thus
\begin{align*}
&p(Y_t = y_t|Y_{-t} = y_{-t}, X_{1:T} = x_{1:T}) \\
&= p(Y_t = y_t|Y_{t-1}=y_{t-1},Y_{t+1} = y_{t+1}, X_{t}=x_{t})\\
&\propto p(Y_t = y_t, Y_{t+1} = y_{t+1}, X_t = x_t | Y_{t-1} = y_{t-1})\\
&= p(Y_{t+1} = y_{t+1}, X_t = x_t | Y_t = y_t, Y_{t-1} = y_{t-1})p(Y_t = y_t|Y_{t-1} = y_{t-1}) \\
&=p(Y_{t+1}=y_{t+1}|Y_t = y_t, Y_{t-1} = y_{t-1})p(X_{t}=x_{t}|Y_t = y_t, Y_{t-1} = y_{t-1})p(Y_t = y_t|Y_{t-1} = y_{t-1})\\
&=p(Y_{t+1}=y_{t+1}|Y_t = y_t)p(X_{t}=x_{t}|Y_t = y_t)p(Y_t = y_t|Y_{t-1} = y_{t-1})\\
\Box
\end{align*}
\textbf{b)}

\begin{align*}
&p(Y_1 = y_1 |Y_{-1}=y_{-1}, X_{1:T} = x_{1:T}) \propto p(Y_2 = y_2|Y_1 = y_1)p(X_1 = x_1|Y_1 = y_1)p(Y_1 = y_1)\\
&p(Y_T = y_T|Y_{-T}=y_{-T},X_{1:T} = x_{1:T}) \propto p(X_T = x_T|Y_T = y_T)p(Y_T = y_T|Y_{T-1} = y_{T-1})p(\langle EOS\rangle|Y_T = y_T)
\end{align*}

\textbf{c)}

The implementation of Gibbs sampling is in \texttt{gibbs\_run.py}. Although the homework mentioned there's no need
to smooth the transition probability, I notice there are some transitions that is missing in the training set (e.g., PART 
can only transit to 16 tags as next state), and I smooth the missing transition by setting 1 as the count.

\textbf{d)}
\Cref{tab:gibbs} shows the prediction accuracy for different $k$ values
\begin{table}
\centering
\begin{tabular}{c|c}
\textbf{$k$}&\textbf{Accuracy}\\\hline
1 & 0.7900\\\hline
2 & 0.8393\\\hline
5 & 0.8430\\\hline
10 & 0.8448\\\hline
100 & 0.8468\\\hline
500 & 0.8450\\\hline
1000 & 0.8447\\\hline
2000 & 0.8477 \\
\end{tabular}
\caption{Gibbs Accuracy for different $k$ value}
\label{tab:gibbs}
\end{table}

\textbf{e)}
The implementation is in \texttt{gibbs\_run\_beta.py} and the result is shown in \Cref{tab:gibbsbeta}

\begin{table}
\centering
\begin{tabular}{c|c|c}
\textbf{$k$} & \textbf{$\beta = 0.5$} & \textbf{$\beta=2.5$} \\\hline
1 & 0.7546 & 0.8031 \\\hline
2 & 0.8047 & 0.8527\\\hline
5 & 0.8044 & 0.8555 \\\hline
10 & 0.8049 & 0.8582 \\\hline
100 & 0.8069 & 0.8591 \\\hline
500 & 0.8048 & 0.8600 \\\hline
1000 & 0.8025 & 0.8586\\\hline
2000 & 0.8054 & 0.8596\\ 
\end{tabular}
\caption{Gibbs Accuracy for different $\beta$}
\label{tab:gibbsbeta}
\end{table}

\textbf{f)}
The implementation is in \texttt{gibbs\_run\_annealing.py} for $\beta = 0.1 + 0.1k_i$, where $k_i$ is 
the number of iterations. I also tried another schedule that set $\beta = 0.1 + 0.05x^2$. Both results are shown 
in \Cref{tab:gibbsanneal}.

\begin{table}
\centering
\begin{tabular}{c|c|c}
\textbf{$k$} & \textbf{$\beta = 0.1k+0.1$} & \textbf{$\beta=0.05k^2+0.1$} \\\hline
1 & 0.6713 & 0.6411 \\\hline
2 & 0.6961 & 0.6624\\\hline
5 & 0.8004 & 0.8374 \\\hline
10 & 0.8432 & 0.8592 \\\hline
100 & 0.8607 &  0.8606\\\hline
500 & 0.8613 &  0.8600\\\hline
1000 & 0.8611 & 0.8600 \\\hline
2000 & 0.8610 & 0.8600 \\
\end{tabular}
\caption{Gibbs Accuracy for different Annealing}
\label{tab:gibbsanneal}
\end{table}

\subsection{Extra Credit: Gibbs Sampling for Minimum Bayes risk Decoding}
\textbf{a)}

When using 0-1 cost, the cost is non-zero only when $y_{1:T} = y'_{1:T}$, thus Equation (7) can be reduced to Equation (6) 
as following
\begin{align*}
\hat y_{1:T} &= \argmin_{y_{1:T}} \sum_{y'_{1:T}}p(Y_{1:T} = y'_{1:T} | X_{1:T} = x_{1:T})\text{cost}(y_{1:T}, y'_{1:T}) \\
&=\argmin_{y_{1:T}}\left[p(Y_{1:T} = y_{1:T} | X_{1:T} = x_{1:T}) + 0\cdot\sum_{y'_{1:T} \neq y_{1:T}}p(Y_{1:T} = y'_{1:T} | X_{1:T} = x_{1:T})\right]\\
&=\argmin_{y_{1:T}}p(Y_{1:T} = y_{1:T} | X_{1:T} = x_{1:T})
\end{align*}

\textbf{b)}

Idea: collect the states $s_1, s_2,\hdots,s_k$ from each iteration during Gibbs Sampling, these are samples of the random variable $Y_{1:T}$ given $X_{1:T} = x_{1:T}$. Thus we can approximate $p(Y_t = y|X_{1:T}=x_{1:T})$ by computing the ratio of states that having
the t-th element to be $y$. The $y$ with highest ratio is then chosen as the result.
\begin{align*}
p(Y_t = y|X_{1:T}=x_{1:T}) &= \sum_{y_{-1}}p(Y_t = y, Y_{-t} = y_{-t}|X_{1:T} =x_{1:T})\\
&=\frac{1}{k}\sum_{s_i, s_i[t] = y} 1
\end{align*}

I perform the experiment with $\beta = 1$, $\beta = 0.1k+0.1$ and $\beta = 0.1k^2+0.1$, the results for different $k$ is demonstrated in \Cref{tab:mbr}

\begin{table}
\centering
\begin{tabular}{c|c|c|c}
\textbf{$k$} &$\beta=1$ &\textbf{$\beta = 0.1k+0.1$} & \textbf{$\beta=0.1k^2+0.1$} \\\hline
1 & & & \\\hline
2 & & &\\\hline
5 & & & \\\hline
10 & & & \\\hline
100 & &  &\\\hline
500 & &  &\\\hline
1000 &  & & \\\hline
2000 &  & & \\
\end{tabular}
\caption{MBR Accuracy for different $\beta$}
\label{tab:mbr}
\end{table}
\end{document}
